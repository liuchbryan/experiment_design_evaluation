{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the theoretical values derived for various experiment designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pedeval.experiment_design import AllSampleBRRED, QualifiedOnlyBRRED\n",
    "from pedeval.evaluation import (\n",
    "    BootstrapMeanEvaluation, EDActualEffectEvaluation, EDMDESizeEvaluation) \n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_collection(eval_collection: List[BootstrapMeanEvaluation], \n",
    "                               in_dir: str = './output',\n",
    "                               expt_design_name: str = 'default',\n",
    "                               quantity_name: str = 'default') -> None:\n",
    "    \"\"\"\n",
    "    Save given `eval_collection` as a pickle file in `in_dir`\n",
    "    :param eval_collection: List of evaluations\n",
    "    :param in_dir: Output directory\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if eval_collection is None or len(eval_collection) == 0:\n",
    "        return\n",
    "\n",
    "    file_path = f\"{in_dir}/{expt_design_name}_{quantity_name}_{int(time.time())}.pickle\"\n",
    "    pickle_file = open(file_path, 'wb')\n",
    "    pickle.dump(eval_collection, pickle_file)\n",
    "\n",
    "    print(f\"The test collection is saved at {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design 2 (Actual Effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUN = 1000\n",
    "\n",
    "design2_actual_effect_evaluations = []\n",
    "for run in range(0, N_RUN):\n",
    "    print(f\"Processing run {run+1}/{N_RUN}...\", end=\"\\r\")\n",
    "\n",
    "    design2 = (\n",
    "        AllSampleBRRED(\n",
    "            p_C0=np.random.uniform(0, 1),\n",
    "            p_C1=np.random.uniform(0, 1),\n",
    "            p_I1=np.random.uniform(0, 1),\n",
    "            p_C2=np.random.uniform(0, 1),\n",
    "            p_I2=np.random.uniform(0, 1),\n",
    "            p_C3=np.random.uniform(0, 1),\n",
    "            p_Iphi=np.random.uniform(0, 1),\n",
    "            p_Ipsi=np.random.uniform(0, 1),\n",
    "            n_0=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_1=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_2=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_3=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            alpha=0.05, pi_min=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    design2_actual_effect_evaluation = (\n",
    "        EDActualEffectEvaluation(design2, n_init_samples=1000, n_bootstrap_mean_samples=200))\n",
    "    design2_actual_effect_evaluation.run()\n",
    "    design2_actual_effect_evaluations.append(design2_actual_effect_evaluation)\n",
    "\n",
    "save_evaluation_collection(design2_actual_effect_evaluations, \n",
    "                           in_dir='./output', expt_design_name=\"all_sample\",\n",
    "                           quantity_name=\"AE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([e.theoretical_value_within_centred_CI(0.05) \n",
    "          for e in design2_actual_effect_evaluations]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design 2 (MDE Size)\n",
    "\n",
    "Warning: Long run-time process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUN = 10\n",
    "\n",
    "design2_mde_size_evaluations = []\n",
    "for run in range(0, N_RUN):\n",
    "    print(f\"Processing run {run+1}/{N_RUN}...\", end=\"\\r\")\n",
    "\n",
    "    design2 = (\n",
    "        AllSampleBRRED(\n",
    "            p_C0=np.random.uniform(0, 1),\n",
    "            p_C1=np.random.uniform(0, 1),\n",
    "            p_I1=np.random.uniform(0, 1),\n",
    "            p_C2=np.random.uniform(0, 1),\n",
    "            p_I2=np.random.uniform(0, 1),\n",
    "            p_C3=np.random.uniform(0, 1),\n",
    "            p_Iphi=np.random.uniform(0, 1),\n",
    "            p_Ipsi=np.random.uniform(0, 1),\n",
    "            n_0=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_1=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_2=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_3=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            alpha=0.05, pi_min=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    design2_mde_size_evaluation = (\n",
    "        EDMDESizeEvaluation(design2, n_init_samples=100, n_bootstrap_mean_samples=1000))\n",
    "    design2_mde_size_evaluation.run(verbose=True)\n",
    "    design2_mde_size_evaluations.append(design2_mde_size_evaluation)\n",
    "\n",
    "save_evaluation_collection(design2_mde_size_evaluations, \n",
    "                           in_dir='./output', expt_design_name=\"all_sample\",\n",
    "                           quantity_name=\"MDES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design 3 (Actual effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUN = 1000\n",
    "\n",
    "design3_actual_effect_evaluations = []\n",
    "for run in range(0, N_RUN):\n",
    "    print(f\"Processing run {run+1}/{N_RUN}...\", end=\"\\r\")\n",
    "\n",
    "    design3 = (\n",
    "        QualifiedOnlyBRRED(\n",
    "            p_C0=np.random.uniform(0, 1),\n",
    "            p_C1=np.random.uniform(0, 1),\n",
    "            p_I1=np.random.uniform(0, 1),\n",
    "            p_C2=np.random.uniform(0, 1),\n",
    "            p_I2=np.random.uniform(0, 1),\n",
    "            p_C3=np.random.uniform(0, 1),\n",
    "            p_Iphi=np.random.uniform(0, 1),\n",
    "            p_Ipsi=np.random.uniform(0, 1),\n",
    "            n_0=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_1=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_2=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_3=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            alpha=0.05, pi_min=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    design3_actual_effect_evaluation = (\n",
    "        EDActualEffectEvaluation(design3, n_init_samples=1000, n_bootstrap_mean_samples=200))\n",
    "    design3_actual_effect_evaluation.run()\n",
    "    design3_actual_effect_evaluations.append(design3_actual_effect_evaluation)\n",
    "\n",
    "save_evaluation_collection(design3_actual_effect_evaluations, \n",
    "                           in_dir='./output', expt_design_name=\"qualified_only\",\n",
    "                           quantity_name=\"AE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([e.theoretical_value_within_centred_CI(0.05) \n",
    "          for e in design3_actual_effect_evaluations]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design 3 (MDE Size)\n",
    "\n",
    "Warning: Long run-time process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUN = 10\n",
    "\n",
    "design3_mde_size_evaluations = []\n",
    "for run in range(0, N_RUN):\n",
    "    print(f\"Processing run {run+1}/{N_RUN}...\", end=\"\\r\")\n",
    "\n",
    "    design3 = (\n",
    "        QualifiedOnlyBRRED(\n",
    "            p_C0=np.random.uniform(0, 1),\n",
    "            p_C1=np.random.uniform(0, 1),\n",
    "            p_I1=np.random.uniform(0, 1),\n",
    "            p_C2=np.random.uniform(0, 1),\n",
    "            p_I2=np.random.uniform(0, 1),\n",
    "            p_C3=np.random.uniform(0, 1),\n",
    "            p_Iphi=np.random.uniform(0, 1),\n",
    "            p_Ipsi=np.random.uniform(0, 1),\n",
    "            n_0=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_1=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_2=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            n_3=int(50 * 10**np.random.uniform(0, 3)),\n",
    "            alpha=0.05, pi_min=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "    design3_mde_size_evaluation = (\n",
    "        EDMDESizeEvaluation(design3, n_init_samples=100, n_bootstrap_mean_samples=1000))\n",
    "    design3_mde_size_evaluation.run(verbose=True)\n",
    "    design3_mde_size_evaluations.append(design3_mde_size_evaluation)\n",
    "\n",
    "save_evaluation_collection(design3_mde_size_evaluations, \n",
    "                           in_dir='./output', expt_design_name=\"qualified_only\",\n",
    "                           quantity_name=\"MDES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
